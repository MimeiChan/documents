// -----------------------------------------------------------------------------
//  CrawlerConsole PoC - 静的 → 動的フォールバック方式（コメント強化版）
//  .NET Framework 4.8 コンソールアプリ
// -----------------------------------------------------------------------------
//  ● 目的
//      東証上場企業トップページを起点に、キーワード＋PDFリンクを含むページ URL を収集する。
//  ● 特徴
//      1) 静的 HTML 解析 → ヒット無ければ Selenium で動的レンダリング
//      2) robots.txt を簡易チェックし、許可されないパスはクロールしない
//      3) 直列 BFS（Queue）でクロールするのでジュニアでも追いやすい構造
//      4) 結果は CSV 出力。DB 連携は PoC ではスコープ外。
//  ● 依存 NuGet
//      HtmlAgilityPack, Newtonsoft.Json, Selenium.WebDriver, Selenium.WebDriver.ChromeDriver
// -----------------------------------------------------------------------------

using System;
using System.IO;
using System.Net.Http;
using System.Collections.Generic;
using System.Linq;
using System.Text.RegularExpressions;
using System.Threading;
using HtmlAgilityPack;
using Newtonsoft.Json;
using OpenQA.Selenium;
using OpenQA.Selenium.Chrome;

namespace CrawlerConsole
{
    // =========================================================================
    //  Program : エントリポイント
    // =========================================================================
    internal static class Program
    {
        // 入力・出力ファイルのパス（固定文字列で十分）
        private const string CompanyCsvPath = "company_urls.csv";
        private const string ConfigPath     = "crawler.json";
        private const string OutputDir      = "output";

        private static void Main()
        {
            // ★ 1. 設定ファイル・企業 CSV を読み込む ----------------------------
            var cfg       = CrawlerConfig.Load(ConfigPath);
            var companies = CompanyLoader.Load(CompanyCsvPath);

            // ★ 出力フォルダが無ければ作成
            Directory.CreateDirectory(OutputDir);
            ResultWriter.Init(OutputDir);

            // ★ robots.txt チェッカーを生成（User-Agent を渡す）
            var robotsChecker = new RobotsTxtChecker(cfg.UserAgent);

            // ★ 2. 企業ごとにクロール ------------------------------------------
            foreach (var comp in companies)
            {
                Console.WriteLine($"\n=== {comp.SecuritiesCode} : {comp.TopPageUrl} ===");

                bool hit = Crawler.Run(comp, cfg, robotsChecker);

                if (!hit)
                {
                    ResultWriter.WriteNoHit(comp);
                    Console.WriteLine("   → ヒット無し");
                }

                Thread.Sleep(1000); // ★ 企業間で 1 秒休止し負荷を下げる
            }

            Console.WriteLine("\nクローリング完了。Enter で終了");
            Console.ReadLine();
        }
    }

    // =========================================================================
    //  CrawlerConfig : 設定モデル
    // =========================================================================
    internal sealed class CrawlerConfig
    {
        // 深度・キーワード・UA などは外部 JSON で自由に変更可能
        public int MaxDepth { get; set; } = 3;
        public List<string> Keywords { get; set; } = new();

        // ★ Headless や “bot” を含まない UA にして WAF 検知を回避
        public string UserAgent { get; set; } =
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 CrawlerForIR/1.0";

        public RenderConfig Render { get; set; } = new();

        // ★ JSON → オブジェクト変換
        public static CrawlerConfig Load(string path) =>
            JsonConvert.DeserializeObject<CrawlerConfig>(File.ReadAllText(path));
    }

    // 描画関連のサブ設定
    internal sealed class RenderConfig
    {
        public string Mode { get; set; } = "Auto"; // None / Auto / Always
        public int MinScriptTags { get; set; } = 5;
        public int WaitMilliseconds { get; set; } = 3000;
    }

    // =========================================================================
    //  Company : 企業モデル
    // =========================================================================
    internal sealed class Company
    {
        public string SecuritiesCode { get; }
        public Uri    TopPageUrl     { get; }

        public Company(string code, Uri url)
        {
            SecuritiesCode = code;
            TopPageUrl     = url;
        }
    }

    // =========================================================================
    //  CompanyLoader : 証券コード & URL の CSV 読込
    // =========================================================================
    internal static class CompanyLoader
    {
        public static List<Company> Load(string csvPath)
        {
            return File.ReadLines(csvPath)
                       .Select(l => l.Trim())
                       .Where(l => l.Contains(','))     // ★ 安全ガード
                       .Select(l => l.Split(','))
                       .Where(p => p.Length >= 2 &&
                                   Uri.TryCreate(p[1].Trim(), UriKind.Absolute, out _))
                       .Select(p => new Company(p[0].Trim(),
                                                new Uri(p[1].Trim())))
                       .ToList();
        }
    }

    // =========================================================================
    //  RobotsTxtChecker : robots.txt の Disallow を見るだけの簡易版
    // =========================================================================
    internal sealed class RobotsTxtChecker
    {
        private readonly HttpClient _http = new();
        private readonly string _ua;
        private readonly Dictionary<string, List<string>> _cache = new(); // ドメインごとにキャッシュ

        public RobotsTxtChecker(string ua) => _ua = ua;

        /// <summary>URL がクロール許可かを返す。キャッシュ込みで高速。</summary>
        public bool IsAllowed(Uri url)
        {
            if (!_cache.TryGetValue(url.Host, out var dis))
            {
                dis = Fetch(url.Host);
                _cache[url.Host] = dis;
            }
            // ★ パスが Disallow にマッチすれば NG
            return dis.All(p => !url.AbsolutePath
                                   .StartsWith(p, StringComparison.OrdinalIgnoreCase));
        }

        // robots.txt をダウンロードし、User-agent:* セクションだけ解析
        private List<string> Fetch(string host)
        {
            try
            {
                var req = new HttpRequestMessage(HttpMethod.Get, $"https://{host}/robots.txt");
                req.Headers.UserAgent.ParseAdd(_ua);
                string txt = _http.Send(req).Content.ReadAsStringAsync().Result;

                var disallow = new List<string>();
                bool starSection = false;

                foreach (var raw in txt.Split('\n'))
                {
                    var line = raw.Trim();

                    if (line.StartsWith("User-agent", StringComparison.OrdinalIgnoreCase))
                    {
                        // ★ 「*」セクションかどうかを判定
                        starSection = line.Contains("*");
                        continue;
                    }

                    if (starSection &&
                        line.StartsWith("Disallow", StringComparison.OrdinalIgnoreCase))
                    {
                        string path = line.Split(':')[1].Trim();
                        if (path.Length > 0) disallow.Add(path);
                    }
                }
                return disallow;
            }
            catch
            {
                // エラー → 「制限なし扱い」で返す（必要ならログ出力）
                return new();
            }
        }
    }

    // =========================================================================
    //  Crawler : BFS クロール本体
    // =========================================================================
    internal static class Crawler
    {
        /// <summary>
        /// 1 社分を深度優先ではなく「幅優先（BFS）」でクロール。
        ///   - 同一 URL への再訪問を防ぐため visited ハッシュを使う
        ///   - robots.txt で禁止されているパスはスキップ
        ///   - 静的 →（必要なら）動的レンダリング
        /// </summary>
        public static bool Run(Company comp,
                               CrawlerConfig cfg,
                               RobotsTxtChecker robots)
        {
            // ★ キーワードを OR 連結で Regex 化
            var kwRegex = new Regex(string.Join("|", cfg.Keywords),
                                    RegexOptions.IgnoreCase |
                                    RegexOptions.Compiled);

            var queue   = new Queue<(Uri url, int depth)>();
            var visited = new HashSet<string>();

            // トップページをキューに詰めてスタート
            queue.Enqueue((comp.TopPageUrl, 0));
            bool foundAny = false;

            while (queue.Count > 0)
            {
                var (url, depth) = queue.Dequeue();

                // 既に回った URL は無視
                if (!visited.Add(url.AbsoluteUri)) continue;

                // robots.txt で禁止ならスキップ
                if (!robots.IsAllowed(url))
                {
                    Console.WriteLine($"   [SKIP:robots] {url}");
                    continue;
                }

                // --------------------------------------------------
                // 1) 静的 HTML 取得
                // --------------------------------------------------
                string html = HtmlGetter.FetchStatic(url, cfg.UserAgent);
                bool hitStatic = PageAnalyzer.IsHit(html, kwRegex);

                // --------------------------------------------------
                // 2) 動的取得が必要か判定
                // --------------------------------------------------
                bool needDynamic = cfg.Render.Mode.Equals("Always",
                                       StringComparison.OrdinalIgnoreCase) ||
                                   (cfg.Render.Mode.Equals("Auto",
                                       StringComparison.OrdinalIgnoreCase) &&
                                    !hitStatic &&
                                    PageAnalyzer.CountScriptTags(html) >=
                                        cfg.Render.MinScriptTags);

                if (needDynamic)
                {
                    html = HtmlGetter.FetchDynamic(url, cfg.Render, cfg.UserAgent);
                }

                // --------------------------------------------------
                // 3) キーワード＋PDFリンク判定
                // --------------------------------------------------
                if (PageAnalyzer.IsHit(html, kwRegex))
                {
                    ResultWriter.WriteHit(comp, url);
                    Console.WriteLine($"   [HIT] {url}");
                    foundAny = true;
                }

                // --------------------------------------------------
                // 4) MaxDepth までリンクを展開
                // --------------------------------------------------
                if (depth < cfg.MaxDepth)
                {
                    foreach (var link in PageAnalyzer.ExtractLinks(html, url))
                        queue.Enqueue((link, depth + 1));
                }
            }

            return foundAny;
        }
    }

    // =========================================================================
    //  HtmlGetter : HTML 取得用ユーティリティ
    // =========================================================================
    internal static class HtmlGetter
    {
        private static readonly HttpClient Http = new();

        /// <summary>静的 GET 取得。例外時は空文字。</summary>
        public static string FetchStatic(Uri url, string ua)
        {
            try
            {
                var req = new HttpRequestMessage(HttpMethod.Get, url);
                req.Headers.UserAgent.ParseAdd(ua);
                return Http.Send(req).Content.ReadAsStringAsync().Result;
            }
            catch (Exception ex)
            {
                Console.WriteLine($"   [ERROR:STATIC] {ex.Message}");
                return string.Empty;
            }
        }

        /// <summary>動的（Selenium）取得。例外時は空文字。</summary>
        public static string FetchDynamic(Uri url, RenderConfig r, string ua)
        {
            try
            {
                var opt = new ChromeOptions();
                opt.AddArgument("--headless");
                opt.AddArgument("--disable-gpu");
                opt.AddArgument("--no-sandbox");
                opt.AddArgument("--blink-settings=imagesEnabled=false"); // ★ 画像読込オフで高速化
                opt.AddArgument($"--user-agent={ua}");

                using IWebDriver drv = new ChromeDriver(opt);
                drv.Navigate().GoToUrl(url);
                Thread.Sleep(r.WaitMilliseconds); // ★ DOM が安定するまで待機
                return drv.PageSource;
            }
            catch (Exception ex)
            {
                Console.WriteLine($"   [ERROR:DYNAMIC] {ex.Message}");
                return string.Empty;
            }
        }
    }

    // =========================================================================
    //  PageAnalyzer : HTML 解析 (キーワード判定・リンク抽出)
    // =========================================================================
    internal static class PageAnalyzer
    {
        // href="xxxxx.pdf" を検出する簡易正規表現
        private static readonly Regex PdfLinkRegex =
            new Regex("href\\s*=\\s*\"[^\"]+\\.pdf\"",
                      RegexOptions.IgnoreCase |
                      RegexOptions.Compiled);

        /// <summary>
        /// キーワードが本文にあり、かつ PDF リンクがあるか？
        /// </summary>
        public static bool IsHit(string html, Regex kwRegex)
        {
            if (string.IsNullOrEmpty(html)) return false;

            bool hasKeyword  = kwRegex.IsMatch(html);      // キーワードがあるか
            bool hasPdfLink  = PdfLinkRegex.IsMatch(html); // PDFリンクがあるか

            return hasKeyword && hasPdfLink;
        }

        /// <summary>`<script>` タグの数で「動的かどうか」をおおまかに判定</summary>
        public static int CountScriptTags(string html)
        {
            if (string.IsNullOrEmpty(html)) return 0;
            var doc = new HtmlDocument();
            doc.LoadHtml(html);
            return doc.DocumentNode
                     .SelectNodes("//script")
                     ?.Count ?? 0;
        }

        /// <summary>同一ドメイン内のリンクを絶対 URL で列挙</summary>
        public static IEnumerable<Uri> ExtractLinks(string html, Uri baseUri)
        {
            if (string.IsNullOrEmpty(html)) yield break;

            var doc = new HtmlDocument();
            doc.LoadHtml(html);
            var nodes = doc.DocumentNode.SelectNodes("//a[@href]");
            if (nodes == null) yield break;

            foreach (var a in nodes)
            {
                string href = a.GetAttributeValue("href", string.Empty)
                               .Split('#')[0];      // #アンカーは除去
                if (href.Length == 0) continue;

                // 相対 → 絶対 URL に解決
                if (!Uri.TryCreate(baseUri, href, out var abs)) continue;

                // 同一ドメインのみ
                if (abs.Host != baseUri.Host) continue;

                yield return abs;
            }
        }
    }

    // =========================================================================
    //  ResultWriter : CSV 出力ユーティリティ
    // =========================================================================
    internal static class ResultWriter
    {
        private static string _hitPath, _noPath;
        private static readonly object _lock = new(); // ★ 直列だが将来の並列化を考慮

        public static void Init(string dir)
        {
            _hitPath = Path.Combine(dir, "ReportPages.csv");
            _noPath  = Path.Combine(dir, "NoHitCompanies.csv");

            // ヘッダー行を書き込む・都度上書き
            File.WriteAllText(_hitPath, "SecuritiesCode,PageUrl\r\n");
            File.WriteAllText(_noPath , "SecuritiesCode\r\n");
        }

        public static void WriteHit(Company c, Uri url)
        {
            lock (_lock)
            {
                File.AppendAllText(_hitPath,
                                   $"{c.SecuritiesCode},{url}\r\n");
            }
        }

        public static void WriteNoHit(Company c)
        {
            lock (_lock)
            {
                File.AppendAllText(_noPath,
                                   $"{c.SecuritiesCode}\r\n");
            }
        }
    }
}
