# 統合報告書自動ダウンロード構想

## 目標

理想はダウンロード率100%だが、機械的判断では不可能な部分が有るためユーザー介入必須

## ダウンロードまでの構想

対象全企業のホームページのカスタム定義を作成する。
主に以下を定義する。

1.  ダウンロードページのURL
2.  ダウンロードリンクのXPath

ダウンロードリンクまでのXPathを人間が定義することで、ダウンロードツールがXPathを参照して確実な自動ダウンロードを実現する。
前提としてこの定義は定期的、もしくはダウンロードの失敗などを検知することで人間が随時最新状態にメンテナンスすることを前提とする。

### 上記仕組みでダウンロード可能なサイト

* 常に同じXPathに最新ファイルがアップされること
    例：pronexus
    https://www.pronexus.co.jp/ir/library/integration/

### 上記仕組みでは問題のあるサイト

* **A. 異なるXPathに最新ファイルがアップされるサイト**
    * 最新版がアップされたという検知自体が難しく、ユーザー通知
* **B. ファイルリンクが動的に表示されたり生成されたりするような仕組みであること(javascript,reactなどのフレームワークで作成されているようなサイト)**
    * 画面内の遷移フロー(どの要素をクリックするかなど)も定義化してSeleniumやPlayWrightなどでブラウザ操作を自動化すればいけるかも？
* **C. サイトリニューアル(どうしようもない。エラー検出で人間が定義を更新する)**

## その他の問題点

* **D. ファイルが更新されているかどうかを検知する仕組みをどうするか**
    * 同じXPathに最新版がアップされる前提であれば、PDFのメタ情報やハッシュ値の比較でいけそう
* **E. ブラウザ自動化利用時の認証リスク**
    * seleniumなどのブラウザ自動化を使った場合、企業によってはcapchaなどの認証リスクが出てくるかもしれない
* **F. CMSごとの解析ノウハウの活用**
    * CMSによっては特定の構造が存在する場合があり、それらを解析・利用することでダウンロードの精度や効率を上げられる可能性があるかも
