Docker内のOllama + OpenWebUI環境でローカルLLMのコンテキスト長を確認・変更する方法ですね。いくつか方法があります。

コンテキスト長は、主に以下の要素で決まります。

1.  **モデル自体の最大コンテキスト長:** モデルが学習時に設計された最大の長さ。これを超えることはできません。
2.  **Ollamaでの設定値:** Ollamaがモデルをロードする際に指定するコンテキスト長。モデルの最大値以下の値を設定できます。

**1. 現状のコンテキスト長の確認方法**

* **モデルの情報を確認する (最も確実):**
    Ollamaが動作しているDockerコンテナ内で、以下のコマンドを実行してモデルの詳細情報を表示します。`Modelfile` の内容が表示され、その中の `PARAMETER num_ctx` や類似のパラメータで設定されているコンテキスト長を確認できます。（`num_ctx` が設定されていない場合、Ollamaのデフォルト値やモデル固有のデフォルトが使われます。）

    ```bash
    # コンテナ内に入る場合 (ollamaコンテナの名前が 'ollama' の場合)
    docker exec -it ollama ollama show <モデル名> --modelfile

    # 例: llama3 モデルの場合
    docker exec -it ollama ollama show llama3 --modelfile
    ```
    `<モデル名>` には `llama3:8b`, `mistral` など、確認したいモデル名を指定してください。

* **モデルのドキュメントを確認する:**
    Ollama HubやHugging Faceなどで、使用しているモデルのドキュメント（モデルカード）を確認すると、そのモデルが対応している最大コンテキスト長が記載されていることが多いです。これはOllamaでの設定値ではなく、モデル自体の限界値です。

* **OpenWebUIの設定画面を確認する:**
    OpenWebUIによっては、モデルごとの設定項目やチャット時の詳細設定で、Ollamaに渡すパラメータ（コンテキスト長を含む）を確認・設定できる場合があります。ただし、UIに表示されていなくてもOllama側で設定されている可能性があります。

**2. コンテキスト長の設定変更方法**

いくつかのレベルで設定変更が可能です。

* **方法1: Ollamaの起動時環境変数で設定する (サーバー全体のデフォルト)**
    OllamaのDockerコンテナを起動する際に、環境変数 `OLLAMA_NUM_CTX` を設定することで、そのOllamaサーバーで実行されるすべてのモデルに対するデフォルトのコンテキスト長を指定できます。

    ```bash
    docker run -d \
      -v ollama:/root/.ollama \
      -p 11434:11434 \
      --name ollama \
      -e OLLAMA_NUM_CTX=4096 \
      ollama/ollama
      # ↑ ここでコンテキスト長を 4096 に設定
    ```
    すでにコンテナが起動している場合は、一度コンテナを停止・削除 (`docker stop ollama && docker rm ollama`) してから、上記の環境変数を追加して再作成・起動してください。（ボリューム `ollama` を使っていれば、ダウンロード済みのモデルは保持されます。）

* **方法2: カスタムModelfileを作成して設定する (モデルごとに永続化)**
    特定のモデルに対して、デフォルトとは異なるコンテキスト長を恒久的に設定したい場合に有効です。

    1.  **ベースとなるモデルのModelfileを取得:**
        ```bash
        # コンテナ内で実行
        ollama show <ベースモデル名> --modelfile > Modelfile
        ```
    2.  **Modelfileを編集:**
        取得した `Modelfile` をテキストエディタで開き、`PARAMETER num_ctx <希望の値>` の行を追加または編集します。
        ```modelfile
        FROM <ベースモデル名>
        PARAMETER num_ctx 8192  # ← コンテキスト長を8192に設定
        # 他のパラメータ...
        ```
        **注意:** 設定する値は、モデル自体がサポートする最大コンテキスト長を超えないようにしてください。
    3.  **新しいモデル名でカスタムモデルを作成:**
        ```bash
        # コンテナ内で実行
        ollama create <新しいカスタムモデル名> -f Modelfile
        # 例: ollama create llama3-8k -f Modelfile
        ```
    4.  **OpenWebUIで使用:**
        OpenWebUIのモデル選択で、作成した `<新しいカスタムモデル名>` を選んで使用します。

* **方法3: OpenWebUIの設定で変更する (UIが対応していれば)**
    OpenWebUIのインターフェース上で、モデル設定やチャットの詳細オプションとしてコンテキスト長（`num_ctx` や "Context Length" といった名前）を変更できる場合があります。ここで設定した値は、通常、APIリクエスト時にOllamaに渡され、そのセッションでのみ有効になるか、UIが設定を保存していれば継続的に利用できます。これが最も手軽ですが、UIの実装に依存します。

**重要な注意点:**

* **モデルの限界:** 設定できるコンテキスト長は、あくまでモデル自体が対応している最大値までです。それを超える値を設定しても、エラーになるか無視される可能性があります。
* **リソース消費:** コンテキスト長を長くすると、必要なメモリ（RAM）やVRAM（GPUメモリ）が大幅に増加します。マシンのスペックによっては、長くしすぎると動作が不安定になったり、非常に遅くなったり、メモリ不足でエラーになったりします。
* **Ollamaの再起動:** 環境変数で設定を変更した場合は、Ollamaコンテナの再起動が必要です。Modelfileでカスタムモデルを作成した場合は、Ollamaの再起動は不要ですが、OpenWebUIで新しいモデルを選択する必要があります。

まずは `ollama show <モデル名> --modelfile` で現状を確認し、必要に応じて上記の方法で変更してみてください。手軽さならOpenWebUIの設定、永続的な変更ならカスタムModelfileや環境変数がおすすめです。